{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook does the following:\n",
    "\n",
    "1. **Retrieve Nucleotide Accession IDs and GFF Files**\n",
    "\n",
    "2. **Download GFF3 Files**\n",
    "   - Use a script to download GFF3 files from NCBI\n",
    "\n",
    "3. **Find Overlap Between Genes in EggNOG and GFF Files**\n",
    "   - Define functions to process files and extract unique values.\n",
    "   - Process gene groups to find corresponding GFF files and extract gene features.\n",
    "   - Write results to JSON and log skipped taxa.\n",
    "\n",
    "4. **Download and Prune ASTRAL WoL Tree**\n",
    "   - Prune the tree to include only taxa with gene data.\n",
    "   - Replace leaf names with taxon IDs and write pruned trees to files.\n",
    "\n",
    "5. **Extract Gene Trees of Interest**\n",
    "   - Extract gene trees based on the number of genes and taxa.\n",
    "\n",
    "6. **Prune Gene Trees**\n",
    "\n",
    "7. **Check Taxa Consistency**\n",
    "   - Ensure taxa in WoL tree and gene trees are consistent.\n",
    "   - Prune WoL tree variants to include only taxa in gene trees.\n",
    "\n",
    "8. **Download Genome Sequence Files**\n",
    "   - Download genome sequence files for each taxon in the dataset.\n",
    "\n",
    "9. **Replace Leaf Labels in Gene Trees**\n",
    "   - Replace the first underscore with a dot in leaf labels of gene trees.\n",
    "\n",
    "10. **Create Gene Features TSV**\n",
    "    - Extract gene features for the subset of genes in the gene trees.\n",
    "\n",
    "11. **Create Presence-Absence Matrices**\n",
    "    - Create presence-absence matrices for GLOOME and COUNT.\n",
    "    - Write matrices to TSV and FASTA files.\n",
    "\n",
    "12. **Extract Function Profiles**\n",
    "    - Extract function profiles for NOGs in the dataset from the EggNOG database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we find a subset of the data in EGGNOG, for which we can retrieve nucleotide data\n",
    "# this means that\n",
    "# 1. first we find for which genomes in EggNOG can we find NCBI genome assembly accession IDs\n",
    "# 2. We download the assembly gff files for these acc IDs\n",
    "# 3. We look through the list of genes in EggNOG, which we can find in the gff files.\n",
    "taxonomicID = '1236'\n",
    "data_dir = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Nucleotide Accession IDs and GFF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$taxonomicID\" \"$data_dir\"\n",
    "## extract data for this dataset based on taxonomic ID declared before, using ripgrep\n",
    "rg -w \"^$1\" $2/eggnog6/e6.og2seqs_and_species.tsv > $2$1_og2seqs_and_species.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$taxonomicID\" \"$data_dir\"\n",
    "# make a list of all taxa in *_og2seqs_and_species.tsv and write it to *_all.taxa\n",
    "# the fifth column in this TSV file is a comma-delimited string of taxon IDs\n",
    "awk -F\"\\t\" '{print $5}' $2$1_og2seqs_and_species.tsv | tr ',' '\\n' | sort -n | uniq > $2$1_all.taxa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `src/download_ncbi_genome_sequences.py` to download the gff3 files from NCBI, using multiprocessing. The script will download the files in the `data_dir`\n",
    "    \n",
    "```bash\n",
    "nohup ~/mambaforge/envs/hgt_analyses/bin/python src/download_ncbi_genome_sequences.py -i 1236_all.taxa --to-download gff3  > nohup_gff3_dload.log & disown\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find overlap between genes in EggNOG and genes in the GFF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress SyntaxWarnings in ete3 import using the warnings module\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=SyntaxWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import gffutils\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import csv\n",
    "import ete3\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "all_taxa_filepath = f'{data_dir}{taxonomicID}_all.taxa'\n",
    "nog_members_tsv_filepath = f'{data_dir}{taxonomicID}_og2seqs_and_species.tsv'\n",
    "\n",
    "\n",
    "def process_file(input_file_path, column_index):\n",
    "    # This set will store all unique values from the specified column\n",
    "    all_values = set()\n",
    "\n",
    "    # Open the file and read it line by line\n",
    "    with open(input_file_path, 'r') as fo:\n",
    "        reader = csv.reader(fo, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            # Split the column by commas and add the items to the set\n",
    "            if row[column_index]:  # Ensure the column isn't empty\n",
    "                values = row[column_index].split(',')\n",
    "                all_values.update(value.strip() for value in values)\n",
    "\n",
    "    return all_values\n",
    "\n",
    "\n",
    "unique_values = process_file(nog_members_tsv_filepath, 5)\n",
    "\n",
    "# Write the unique values to a file or process them further\n",
    "with open(f'{data_dir}{taxonomicID}_all.genes', 'w') as out_file:\n",
    "    out_file.write('\\n'.join(sorted(unique_values)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1548/1548 [00:56<00:00, 27.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of taxa with gene data: 853\n"
     ]
    }
   ],
   "source": [
    "# read in the values from the *all.genes file into a df with a single column\n",
    "all_genes_filepath = data_dir + taxonomicID + '_all.genes'\n",
    "\n",
    "all_genes_df = pd.read_csv(\n",
    "    all_genes_filepath, header=None, names=['locus_tag'])\n",
    "\n",
    "taxon_acc_id_filepath = f'{data_dir}/genome_data/{taxonomicID}_all_accession_ids.tsv'\n",
    "genome_data_dir = f'{data_dir}/genome_data/'\n",
    "\n",
    "taxon_acc_id_df = pd.read_csv(taxon_acc_id_filepath, sep='\\t', header=0)\n",
    "taxon_acc_id_df['taxon_id'] = taxon_acc_id_df['taxon_id'].astype(str)\n",
    "\n",
    "# add a column called \"taxon_id\" based on the first element in the period delimited 'locus_tag' column string\n",
    "all_genes_df['taxon_id'] = all_genes_df['locus_tag'].apply(\n",
    "    lambda x: x.split('.', 1)[0]).astype(str)\n",
    "\n",
    "# groupby taxon and for every taxon, run this fn\n",
    "# this fn takes finds the corresponding gff file using the taxon to gff mapping in taxon_acc_id_df\n",
    "# it then looks for the locus_tag values in the gff. It returns a list of all locus_tag values that exist for this taxon in the gff file\n",
    "\n",
    "\n",
    "def process_gene_group(gene_group, taxon_acc_id_df, genome_data_dir):\n",
    "    \"\"\"\n",
    "    This function takes a group of genes from the all_genes_df dataframe and processes the corresponding gff file.\n",
    "    It returns a list of gene IDs that are found in the gff file.\n",
    "    \"\"\"\n",
    "    taxon_id = gene_group['taxon_id'].iloc[0]\n",
    "    locus_tags = set(gene_group['locus_tag'])\n",
    "    # remove taxon_id from locus_tags (i.e. the first element in the period delimited 'locus_tag' column string)\n",
    "    locus_tags = {locus_tag.split('.', 1)[1] for locus_tag in locus_tags}\n",
    "\n",
    "    # find corresponding gff file for taxon ID\n",
    "    if taxon_acc_id_df['taxon_id'].isin([taxon_id]).any():\n",
    "        acc_id = taxon_acc_id_df[taxon_acc_id_df['taxon_id']\n",
    "                                 == taxon_id]['acc_id'].values[0]\n",
    "    else:\n",
    "        # print(f'taxon ID {taxon_id} not found. Skipping')\n",
    "        return 'skipped: taxon ID not in TSV', taxon_id\n",
    "    gff_filepath = os.path.join(genome_data_dir, f'{taxon_id}_{acc_id}_genomic.gff')\n",
    "    if not os.path.exists(gff_filepath):\n",
    "        # print(f'gff file {gff_filepath} not found. Skipping')\n",
    "        return 'skipped: gff file not found', taxon_id\n",
    "\n",
    "    # create gffutils df from the gff file\n",
    "    gff_db = gffutils.create_db(gff_filepath, ':memory:', merge_strategy='merge')\n",
    "\n",
    "\n",
    "    # return the list of dict of locus_tag values found in the gff file, the taxon_id, the acc_id, and the gene_features_list\n",
    "    return_dict = {}\n",
    "\n",
    "    # iterate over each feature of type 'gene' in the gff_db\n",
    "    for feature in gff_db.features_of_type('gene'):\n",
    "        # check if 'locus_tag' or 'old_locus_tag' is in feature.attributes and if its value is in locus_tags\n",
    "        if 'locus_tag' in feature.attributes and feature.attributes['locus_tag'][0] in locus_tags:\n",
    "            # if yes, append the locus_tag and the entire feature as a dict, including information of start, end, strand, and feature attributes\n",
    "            return_dict[feature.attributes['locus_tag'][0]] = {\n",
    "                'id': feature.id,  # 'gene:NC_000913:thrL'\n",
    "                'locus_tag': feature.attributes['locus_tag'][0],\n",
    "                'start': feature.start,\n",
    "                'end': feature.end,\n",
    "                'strand': feature.strand,\n",
    "                'attributes': dict(feature.attributes),\n",
    "                'seqid': feature.seqid\n",
    "            }\n",
    "\n",
    "        elif 'old_locus_tag' in feature.attributes and feature.attributes['old_locus_tag'][0] in locus_tags:\n",
    "            # if yes, append to dict\n",
    "            return_dict[feature.attributes['old_locus_tag'][0]] = {\n",
    "                'id': feature.id,\n",
    "                'locus_tag': feature.attributes['old_locus_tag'][0],\n",
    "                'start': feature.start,\n",
    "                'end': feature.end,\n",
    "                'strand': feature.strand,\n",
    "                'attributes': dict(feature.attributes),\n",
    "                'seqid': feature.seqid\n",
    "            }\n",
    "            \n",
    "    # if fewer than 10 genes are found, skip\n",
    "    if len(return_dict) < 10:\n",
    "        return 'skipped: less than 10 genes found', taxon_id\n",
    "\n",
    "    return 'values', taxon_id, acc_id, return_dict\n",
    "\n",
    "\n",
    "def process_gene_group_wrapper(args):\n",
    "    return process_gene_group(*args)\n",
    "\n",
    "# groupby taxon_id the genes in all_genes_df\n",
    "gene_groups = all_genes_df.groupby('taxon_id')\n",
    "\n",
    "# prepare arguments for process_gene_group function\n",
    "args = [(group, taxon_acc_id_df, genome_data_dir) for _, group in gene_groups]\n",
    "\n",
    "taxa_acc_genes_dict = {}\n",
    "skipped_taxa_list = []\n",
    "# create a multiprocessing Pool\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    # apply process_gene_group function to each group in parallel\n",
    "    for pool_return in tqdm.tqdm(pool.imap_unordered(process_gene_group_wrapper, args), total=len(args)):\n",
    "        if pool_return[0].startswith('skipped'):\n",
    "            skipped_taxa_list.append((pool_return[1], pool_return[0]))\n",
    "        elif pool_return[0] == 'values':\n",
    "            taxa_acc_genes_dict[pool_return[1]] = {\n",
    "                'acc_id': pool_return[2], 'genes': pool_return[3]}\n",
    "        else:\n",
    "            print('ERROR: invalid return value')\n",
    "            break\n",
    "\n",
    "# write the results to a json file\n",
    "with open(f'{data_dir}{taxonomicID}_gene_features.json', 'w') as out_file:\n",
    "    json.dump(taxa_acc_genes_dict, out_file, indent=4)\n",
    "# log number of taxa in this json\n",
    "print(f'Number of taxa with gene data: {len(taxa_acc_genes_dict)}')\n",
    "\n",
    "# write the skipped taxa to a file\n",
    "with open(f'{data_dir}{taxonomicID}_skipped_taxa.tsv', 'w') as out_file:\n",
    "    # csv with quotes\n",
    "    writer = csv.writer(out_file, quoting=csv.QUOTE_MINIMAL, delimiter='\\t')\n",
    "    writer.writerows(skipped_taxa_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find which locus_tags are present in the json file of gene_features\n",
    "gene_features_filepath = f'{data_dir}{taxonomicID}_gene_features.json'\n",
    "with open(gene_features_filepath, 'r') as gene_features_file:\n",
    "    gene_features_dict = json.load(gene_features_file)\n",
    "\n",
    "# find which 'genes' are present in the gene_features_dict\n",
    "gene_features_dict_genes = [f'{taxon}.{locus_tag}' for taxon in gene_features_dict.keys() for locus_tag in gene_features_dict[taxon]['genes'].keys()]\n",
    "\n",
    "# write this list to a file\n",
    "with open(f'{data_dir}{taxonomicID}_locus_tags_with_features.txt', 'w') as gene_features_genes_file:\n",
    "    gene_features_genes_file.write('\\n'.join(gene_features_dict_genes) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Prune ASTRAL WoL Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 14:52:45 URL:https://biocore.github.io/wol/data/genomes/metadata.tsv.xz [971240/971240] -> \"../data/wol_metadata.tsv.xz\" [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
      "p7zip Version 16.02 (locale=en_US.UTF8,Utf16=on,HugeFiles=on,64 bits,128 CPUs AMD EPYC 7601 32-Core Processor                 (800F12),ASM,AES-NI)\n",
      "\n",
      "Scanning the drive for archives:\n",
      "1 file, 971240 bytes (949 KiB)\n",
      "\n",
      "Extracting archive: ../data/wol_metadata.tsv.xz\n",
      "--\n",
      "Path = ../data/wol_metadata.tsv.xz\n",
      "Type = xz\n",
      "Physical Size = 971240\n",
      "Method = LZMA2:26 CRC64\n",
      "Streams = 1\n",
      "Blocks = 1\n",
      "\n",
      "Everything is Ok\n",
      "\n",
      "Size:       6601710\n",
      "Compressed: 971240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 14:52:46 URL:https://raw.githubusercontent.com/biocore/wol/master/data/trees/astral/branch_length/cons/astral.cons.nwk [407397/407397] -> \"../data/wol_tree.nwk\" [1]\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$taxonomicID\" \"$data_dir\"\n",
    "# remove existing files if they exist\n",
    "rm -f $2\"wol_metadata.tsv.xz\" $2\"wol_metadata.tsv\" $2\"wol_tree.nwk\"\n",
    "wget -nv https://biocore.github.io/wol/data/genomes/metadata.tsv.xz -O $2\"wol_metadata.tsv.xz\"\n",
    "# this requires 7z\n",
    "7z x $2\"wol_metadata.tsv.xz\" -o$2\n",
    "# download the ASTRAL tree\n",
    "wget -nv https://github.com/biocore/wol/raw/master/data/trees/astral/branch_length/cons/astral.cons.nwk -O $2\"wol_tree.nwk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of genomes in the metadata: 10575\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#genome</th>\n",
       "      <th>asm_name</th>\n",
       "      <th>assembly_accession</th>\n",
       "      <th>bioproject</th>\n",
       "      <th>biosample</th>\n",
       "      <th>wgs_master</th>\n",
       "      <th>seq_rel_date</th>\n",
       "      <th>submitter</th>\n",
       "      <th>ftp_path</th>\n",
       "      <th>img_id</th>\n",
       "      <th>...</th>\n",
       "      <th>coding_density</th>\n",
       "      <th>completeness</th>\n",
       "      <th>contamination</th>\n",
       "      <th>strain_heterogeneity</th>\n",
       "      <th>markers</th>\n",
       "      <th>5s_rrna</th>\n",
       "      <th>16s_rrna</th>\n",
       "      <th>23s_rrna</th>\n",
       "      <th>trnas</th>\n",
       "      <th>draft_quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G000005825</td>\n",
       "      <td>ASM582v2</td>\n",
       "      <td>GCF_000005825.2</td>\n",
       "      <td>PRJNA224116</td>\n",
       "      <td>SAMN02603086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010/12/15</td>\n",
       "      <td>Center for Genomic Sciences, Allegheny-Singer ...</td>\n",
       "      <td>ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000...</td>\n",
       "      <td>646311908</td>\n",
       "      <td>...</td>\n",
       "      <td>85.144124</td>\n",
       "      <td>98.68</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>20</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G000006175</td>\n",
       "      <td>ASM617v2</td>\n",
       "      <td>GCF_000006175.1</td>\n",
       "      <td>PRJNA224116</td>\n",
       "      <td>SAMN00000040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010/06/03</td>\n",
       "      <td>US DOE Joint Genome Institute (JGI-PGF)</td>\n",
       "      <td>ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000...</td>\n",
       "      <td>646564549</td>\n",
       "      <td>...</td>\n",
       "      <td>80.167033</td>\n",
       "      <td>99.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>19</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G000006605</td>\n",
       "      <td>ASM660v1</td>\n",
       "      <td>GCF_000006605.1</td>\n",
       "      <td>PRJNA224116</td>\n",
       "      <td>SAMEA3283089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005/06/27</td>\n",
       "      <td>Bielefeld Univ</td>\n",
       "      <td>ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000...</td>\n",
       "      <td>637000085</td>\n",
       "      <td>...</td>\n",
       "      <td>89.378688</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>319</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>20</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G000006725</td>\n",
       "      <td>ASM672v1</td>\n",
       "      <td>GCF_000006725.1</td>\n",
       "      <td>PRJNA224116</td>\n",
       "      <td>SAMN02603773</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004/06/04</td>\n",
       "      <td>Sao Paulo state (Brazil) Consortium</td>\n",
       "      <td>ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000...</td>\n",
       "      <td>637000348</td>\n",
       "      <td>...</td>\n",
       "      <td>82.592990</td>\n",
       "      <td>99.59</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>325</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>20</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G000006745</td>\n",
       "      <td>ASM674v1</td>\n",
       "      <td>GCF_000006745.1</td>\n",
       "      <td>PRJNA57623</td>\n",
       "      <td>SAMN02603969</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001/01/09</td>\n",
       "      <td>TIGR</td>\n",
       "      <td>ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000...</td>\n",
       "      <td>637000333</td>\n",
       "      <td>...</td>\n",
       "      <td>86.533164</td>\n",
       "      <td>99.86</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>360</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>20</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      #genome  asm_name assembly_accession   bioproject     biosample  \\\n",
       "0  G000005825  ASM582v2    GCF_000005825.2  PRJNA224116  SAMN02603086   \n",
       "1  G000006175  ASM617v2    GCF_000006175.1  PRJNA224116  SAMN00000040   \n",
       "2  G000006605  ASM660v1    GCF_000006605.1  PRJNA224116  SAMEA3283089   \n",
       "3  G000006725  ASM672v1    GCF_000006725.1  PRJNA224116  SAMN02603773   \n",
       "4  G000006745  ASM674v1    GCF_000006745.1   PRJNA57623  SAMN02603969   \n",
       "\n",
       "  wgs_master seq_rel_date                                          submitter  \\\n",
       "0        NaN   2010/12/15  Center for Genomic Sciences, Allegheny-Singer ...   \n",
       "1        NaN   2010/06/03            US DOE Joint Genome Institute (JGI-PGF)   \n",
       "2        NaN   2005/06/27                                     Bielefeld Univ   \n",
       "3        NaN   2004/06/04                Sao Paulo state (Brazil) Consortium   \n",
       "4        NaN   2001/01/09                                               TIGR   \n",
       "\n",
       "                                            ftp_path     img_id  ...  \\\n",
       "0  ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000...  646311908  ...   \n",
       "1  ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000...  646564549  ...   \n",
       "2  ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000...  637000085  ...   \n",
       "3  ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000...  637000348  ...   \n",
       "4  ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000...  637000333  ...   \n",
       "\n",
       "  coding_density completeness contamination strain_heterogeneity markers  \\\n",
       "0      85.144124        98.68          1.32                  0.0     377   \n",
       "1      80.167033        99.05          0.00                  0.0     165   \n",
       "2      89.378688       100.00          0.68                  0.0     319   \n",
       "3      82.592990        99.59          0.18                  0.0     325   \n",
       "4      86.533164        99.86          0.03                  0.0     360   \n",
       "\n",
       "  5s_rrna 16s_rrna  23s_rrna trnas draft_quality  \n",
       "0     yes      yes       yes    20          high  \n",
       "1      no      yes       yes    19        medium  \n",
       "2     yes      yes       yes    20          high  \n",
       "3     yes      yes       yes    20          high  \n",
       "4     yes      yes       yes    20          high  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of genomes after pruning based on taxid in gene_features_dict: 412\n",
      "acc_id_to_keep looks like: ['GCF_000245735.1', 'GCA_001801365.1', 'GCF_000829415.1', 'GCF_000271305.1', 'GCF_001469215.1']\n",
      "Number of genomes in the pruned metadata: 360\n",
      "Number of genomes in the pruned tree: 360\n"
     ]
    }
   ],
   "source": [
    "# prune the ASTRAL WoL tree to only include the taxa for which we have gene data. This is our genome tree\n",
    "# read in the tree\n",
    "\n",
    "wol_metadata_filepath = f'{data_dir}wol_metadata.tsv'\n",
    "wol_tree_filepath = f'{data_dir}wol_tree.nwk'\n",
    "wol_tree = ete3.Tree(wol_tree_filepath, format=1)\n",
    "wol_tree_leaf_names = wol_tree.get_leaf_names()\n",
    "\n",
    "# read in the gene features json\n",
    "json_filepath = f'{data_dir}{taxonomicID}_gene_features.json'\n",
    "with open(json_filepath, 'r') as in_file:\n",
    "    gene_features_dict = json.load(in_file)\n",
    "\n",
    "# Read the metadata file from WOL as a dataframe\n",
    "wol_df = pd.read_csv(wol_metadata_filepath, sep='\\t', header=0)\n",
    "print(f'Number of genomes in the metadata: {len(wol_df)}')\n",
    "# change dtype of taxid column to str type\n",
    "wol_df['taxid'] = wol_df['taxid'].astype(str)\n",
    "# display first 5 rows\n",
    "display(wol_df.head())\n",
    "# check if every #genome in the wol_df maps to a unique assembly_accession value\n",
    "assert wol_df['assembly_accession'].nunique() == wol_df['#genome'].nunique()\n",
    "# smaller wol_df based on which taxid are in the gene_features_dict\n",
    "wol_df = wol_df[wol_df['taxid'].isin(gene_features_dict.keys())]\n",
    "print(f'Number of genomes after pruning based on taxid in gene_features_dict: {len(wol_df)}')\n",
    "\n",
    "# gene_features_dict has taxid to acc_id mapping. wol_df has #genome to acc_id (`assembly_accession` column) mapping. Tree has #genome as leaf names\n",
    "# we want to prune the trees and replace the leaf names with taxid values\n",
    "acc_id_to_keep = set([i['acc_id'] for i in gene_features_dict.values()])\n",
    "print(f'acc_id_to_keep looks like: {list(acc_id_to_keep)[:5]}')\n",
    "# remove all other rows from the wol_df\n",
    "wol_df = wol_df[wol_df['assembly_accession'].isin(acc_id_to_keep)]\n",
    "print(f'Number of genomes in the pruned metadata: {len(wol_df)}')\n",
    "genomes_to_keep = [i for i in wol_tree_leaf_names if i in wol_df['#genome'].values]\n",
    "\n",
    "wol_tree.prune(genomes_to_keep, preserve_branch_length=True)\n",
    "print(f'Number of genomes in the pruned tree: {len(wol_tree.get_leaf_names())}')\n",
    "\n",
    "# create a dictionary mapping genome IDs to assembly accessions IDs\n",
    "genome_to_accid_dict = wol_df.set_index('#genome')['assembly_accession'].to_dict()\n",
    "# create a dictionary mapping assembly accessions to taxon IDs\n",
    "accid_to_taxid_dict = {v['acc_id']: k for k, v in gene_features_dict.items()}\n",
    "# create a dictionary mapping genome IDs to taxon IDs\n",
    "genome_to_taxid_dict = {k: accid_to_taxid_dict[v]\n",
    "                   for k, v in genome_to_accid_dict.items()}\n",
    "\n",
    "# replace pruned tree leaf names with taxon IDs\n",
    "for leaf in wol_tree.get_leaves():\n",
    "    leaf.name = genome_to_taxid_dict[leaf.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node names look like: [Tree node '1896966' (0x7f38b054c0b), Tree node '381306' (0x7f38afdb0b9), Tree node '246195' (0x7f38afe058b), Tree node '797473' (0x7f38afe0597), Tree node 'N1' (0x7f38afe057f)] ...\n",
      "New leaf names look like:  ['1896966', '381306', '246195', '797473', '1766620', '555778', '1033802', '1304275', '1172194', '1579979'] ...\n"
     ]
    }
   ],
   "source": [
    "# write the pruned tree to a newick file, with different variations in labelling for different program requirements\n",
    "pruned_no_internal_wol_tree_filepath = f'{data_dir}{taxonomicID}_wol_tree_pruned_no_internal_labels.nwk'\n",
    "pruned_angst_wol_tree_filepath = f'{data_dir}{taxonomicID}_wol_tree_pruned_angst.nwk'\n",
    "pruned_internal_wol_tree_filepath = f'{data_dir}{taxonomicID}_wol_tree_pruned_with_internal_labels.nwk'\n",
    "\n",
    "# write the pruned tree to a newick file without internal node names\n",
    "wol_tree.write(outfile=pruned_no_internal_wol_tree_filepath, format=1, format_root_node=True, dist_formatter='%.10f')\n",
    "\n",
    "# write a version of the tree with root length for AnGST.\n",
    "# first make a copy of the tree\n",
    "wol_tree.write(outfile=pruned_angst_wol_tree_filepath,  \n",
    "               format=1, dist_formatter='%f', format_root_node=True)\n",
    "# find the last \")\" in this file and replace everything after it with \":0.0);\".\n",
    "# AnGST doesn't like the branch length at the root to be like 0.00000\n",
    "with open(pruned_angst_wol_tree_filepath, \"r\") as fi:\n",
    "    tree_str = fi.read()\n",
    "    tree_str = tree_str[:tree_str.rfind(\")\")+1] + \":0.0);\"\n",
    "with open(pruned_angst_wol_tree_filepath, \"w\") as fo:\n",
    "    fo.write(tree_str)\n",
    "\n",
    "# traverse the tree in postorder and number the internal nodes as N1, N2, N3, etc.\n",
    "# We don't skip a loop index if the node is a leaf. But we don't use the index for the leaf nodes, retaining the original name.\n",
    "# We can use the enumerate function to get the index and the node at the same time.\n",
    "internal_node_index = 1\n",
    "for node in wol_tree.traverse(strategy=\"postorder\"):\n",
    "    if not node.is_leaf():\n",
    "        node.name = f\"N{internal_node_index}\"\n",
    "        internal_node_index += 1\n",
    "# print the first 5 nodes of the tree\n",
    "print(f\"Node names look like: {\n",
    "      [i for i in wol_tree.traverse(strategy='postorder')][:5]} ...\")\n",
    "# show the pruned tree leaf labels\n",
    "print(\"New leaf names look like: \", wol_tree.get_leaf_names()[:10], \"...\")\n",
    "# write this wol tree to a file. Branches are full floating point numbers\n",
    "wol_tree.write(outfile=pruned_internal_wol_tree_filepath, \n",
    "               format=1, dist_formatter='%f', format_root_node=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of taxa in the tree:  360 Number of taxa in the dictionary:  853\n"
     ]
    }
   ],
   "source": [
    "# how many taxa are in the tree and how many in the dictionary?\n",
    "print(\"Number of taxa in the tree: \", len(wol_tree.get_leaves()), \"Number of taxa in the dictionary: \", len(gene_features_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract gene trees of the subset data of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EggNOG 6.0 has one huge file with all the trees as well as (compressed) MSA. We need just the trees, and only for the NOGs that are of interest. Extract them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$taxonomicID\" \"$data_dir\"\n",
    "# use ripgrep to extract the trees for eggnog subset dataset (taxonomic ID should be in second column of TSV file). \n",
    "# Write column 1 and 3 to 2_all_trees.tsv\n",
    "rg \"\\t$1\\t\" $2\"e6.all_raw_trees_and_algs.tsv\" | cut -f1,3 > $2$1\"_all_trees.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of genes to retain a NOG:  10 and min number of taxa to retain a NOG:  30\n",
      "Mean #genes_to_keep:  84.05419908773813 Std dev of #genes_to_keep:  178.40979118790855\n",
      "Starting with n =  100  as ~quarter of the std dev of #genes_to_keep in nog_members_df\n",
      "At n =  1300 , taxon_list is a subset of taxa_in_subset, with total of  360  taxa\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxonomic_group</th>\n",
       "      <th>NOG</th>\n",
       "      <th>#taxa</th>\n",
       "      <th>#genes</th>\n",
       "      <th>taxa</th>\n",
       "      <th>genes</th>\n",
       "      <th>#genes_to_keep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14341</th>\n",
       "      <td>1236</td>\n",
       "      <td>ER3QB</td>\n",
       "      <td>190</td>\n",
       "      <td>221</td>\n",
       "      <td>[1294143, 1798802, 572477, 273526, 1441930, 40...</td>\n",
       "      <td>[105559.Nwat_2444, 1121939.L861_15835, 1178482...</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14363</th>\n",
       "      <td>1236</td>\n",
       "      <td>ER3QZ</td>\n",
       "      <td>229</td>\n",
       "      <td>258</td>\n",
       "      <td>[1926881, 230089, 910964, 69222, 1484157, 1681...</td>\n",
       "      <td>[1005058.UMN179_01181, 1006000.GKAS_02034, 105...</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75558</th>\n",
       "      <td>1236</td>\n",
       "      <td>ESNNU</td>\n",
       "      <td>337</td>\n",
       "      <td>372</td>\n",
       "      <td>[1736612, 216778, 273526, 871585, 1484157, 140...</td>\n",
       "      <td>[1006000.GKAS_04123, 1045855.DSC_01400, 111551...</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17358</th>\n",
       "      <td>1236</td>\n",
       "      <td>ER6B2</td>\n",
       "      <td>254</td>\n",
       "      <td>301</td>\n",
       "      <td>[743720, 717774, 491952, 1178482, 290398, 1445...</td>\n",
       "      <td>[1028989.PSCI_0675, 1051646.IX91_03105, 105164...</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44651</th>\n",
       "      <td>1236</td>\n",
       "      <td>ERVXT</td>\n",
       "      <td>256</td>\n",
       "      <td>273</td>\n",
       "      <td>[1736612, 43263, 1294143, 216778, 1535422, 179...</td>\n",
       "      <td>[1163408.UU9_10362, 1187848.A1QO_01760, 119576...</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       taxonomic_group    NOG  #taxa  #genes  \\\n",
       "14341             1236  ER3QB    190     221   \n",
       "14363             1236  ER3QZ    229     258   \n",
       "75558             1236  ESNNU    337     372   \n",
       "17358             1236  ER6B2    254     301   \n",
       "44651             1236  ERVXT    256     273   \n",
       "\n",
       "                                                    taxa  \\\n",
       "14341  [1294143, 1798802, 572477, 273526, 1441930, 40...   \n",
       "14363  [1926881, 230089, 910964, 69222, 1484157, 1681...   \n",
       "75558  [1736612, 216778, 273526, 871585, 1484157, 140...   \n",
       "17358  [743720, 717774, 491952, 1178482, 290398, 1445...   \n",
       "44651  [1736612, 43263, 1294143, 216778, 1535422, 179...   \n",
       "\n",
       "                                                   genes  #genes_to_keep  \n",
       "14341  [105559.Nwat_2444, 1121939.L861_15835, 1178482...              55  \n",
       "14363  [1005058.UMN179_01181, 1006000.GKAS_02034, 105...              55  \n",
       "75558  [1006000.GKAS_04123, 1045855.DSC_01400, 111551...              55  \n",
       "17358  [1028989.PSCI_0675, 1051646.IX91_03105, 105164...              55  \n",
       "44651  [1163408.UU9_10362, 1187848.A1QO_01760, 119576...              55  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first we decide on which NOGs to keep. \n",
    "\n",
    "# we calculate the mean and in 0.25xSD around the mean, we include NOGs from the distribution of NOG sizes\n",
    "# until the list of NOGs included, includes all the taxa to be included.\n",
    "\n",
    "wol_tree = ete3.Tree(f\"{data_dir}{taxonomicID}_wol_tree_pruned_no_internal_labels.nwk\", format=1)\n",
    "taxon_list = wol_tree.get_leaf_names()\n",
    "taxon_set = set(taxon_list)\n",
    "\n",
    "nog_members_tsv_filepath = f\"{data_dir}{taxonomicID}_og2seqs_and_species.tsv\"\n",
    "nog_members_df = pd.read_csv(nog_members_tsv_filepath, sep='\\t', header=None, names=['taxonomic_group',\n",
    "                                                                                    'NOG',\n",
    "                                                                                    '#taxa',\n",
    "                                                                                    '#genes',\n",
    "                                                                                    'taxa', 'genes'])\n",
    "\n",
    "\n",
    "# pruning preparation\n",
    "min_genes = 10  # minimum number of genes to retain a NOG\n",
    "min_taxa = 30  # minimum number of taxa to retain a NOG\n",
    "print(\"Min number of genes to retain a NOG: \", min_genes,\n",
    "      \"and min number of taxa to retain a NOG: \", min_taxa)\n",
    "\n",
    "# taxa column in members_df contains a list of taxonomic IDs that are members of the NOG, as comma-seperated string\n",
    "# Split the 'taxa' column by commas to create a list of taxa in each NOG, add it to a new column\n",
    "nog_members_df['taxa'] = nog_members_df['taxa'].str.split(',')\n",
    "\n",
    "# for the column 'genes' in nog_members_df, replace the periods with underscores\n",
    "nog_members_df['genes'] = nog_members_df['genes'].str.replace('.', '_')\n",
    "# there are mistakes in the 'gene' column, where some underscores are replaced with colon characters. Replace these with underscores\n",
    "nog_members_df['genes'] = nog_members_df['genes'].str.replace(':', '_')\n",
    "# Split the 'genes' column by commas to create a list of genes in each NOG\n",
    "nog_members_df['genes'] = nog_members_df['genes'].str.split(',')\n",
    "\n",
    "# replace the first underscore in 'genes' with a period, to get the taxon ID\n",
    "nog_members_df['genes'] = nog_members_df['genes'].apply(\n",
    "    lambda genes: [gene.replace('_', '.', 1) for gene in genes])\n",
    "# Filter the list of genes based on the taxon_set\n",
    "# Use the first part of an period-split to check if the taxonomic ID is in the taxon_list, and retain only those genes\n",
    "nog_members_df['genes'] = nog_members_df['genes'].apply(\n",
    "    lambda genes: [gene for gene in genes if gene.split('.')[0] in taxon_set])\n",
    "# Create a new column that contains the number of genes to be retained in the gene tree\n",
    "nog_members_df['#genes_to_keep'] = nog_members_df['genes'].str.len()\n",
    "# remove rows where #genes_to_keep is less than 10 or more than max_genes\n",
    "nog_members_df = nog_members_df[(nog_members_df['#genes_to_keep'] >= min_genes)]\n",
    "# similarly, prune the taxa list column to only keep the taxa that are in the taxon_list\n",
    "nog_members_df['taxa'] = nog_members_df['taxa'].apply(\n",
    "    lambda taxa: [taxon for taxon in taxa if taxon in taxon_set])\n",
    "\n",
    "# Sort the df by the absolute difference between #genes_to_keep and its mean\n",
    "# This will give us the NOGs that are closest to the mean\n",
    "mean_genes_to_keep = nog_members_df['#genes_to_keep'].mean()\n",
    "nog_members_df['diff_from_mean'] = abs(\n",
    "    nog_members_df['#genes_to_keep'] - mean_genes_to_keep)\n",
    "nog_members_df = nog_members_df.sort_values(by='diff_from_mean')\n",
    "\n",
    "# find std dev of #genes_to_keep in members_df\n",
    "nog_members_df_std_dev = nog_members_df['#genes_to_keep'].std()\n",
    "# display mean and std dev\n",
    "print(\"Mean #genes_to_keep: \", nog_members_df['#genes_to_keep'].mean(\n",
    "), \"Std dev of #genes_to_keep: \", nog_members_df_std_dev)\n",
    "\n",
    "# Flatten the list of taxa in the dataframe\n",
    "all_taxa_in_df = set(\n",
    "    [taxon for sublist in nog_members_df['taxa'].tolist() for taxon in sublist])\n",
    "\n",
    "# Check if all taxa in taxon_list are represented in nog_members_df\n",
    "if not set(taxon_list).issubset(all_taxa_in_df):\n",
    "    print(\"Not all taxa in taxon_list are represented in nog_members_df\")\n",
    "    print(\"Taxa not represented: \", set(taxon_list) - all_taxa_in_df)\n",
    "else:\n",
    "\n",
    "    # retain only NOGs with at least min_taxa taxa\n",
    "    nog_members_df = nog_members_df[nog_members_df['#taxa'] >= min_taxa]\n",
    "    \n",
    "    # check if the smallest n NOGs contain all the taxa in taxon_list, if not, increase n by 100 and repeat\n",
    "\n",
    "    # start with a quarter of the std dev of #genes_to_keep in nog_members_df, rounded to closest higher 100\n",
    "    n = int(np.ceil(nog_members_df_std_dev/4/100)*100)\n",
    "    print(\"Starting with n = \", n,\n",
    "          \" as ~quarter of the std dev of #genes_to_keep in nog_members_df\")\n",
    "    while True:\n",
    "        # make sure the loop doesn't run forever\n",
    "        if n > len(nog_members_df):\n",
    "            print(\"n is greater than the number of NOGs in the nog_members_df\")\n",
    "            break\n",
    "        # get the smallest n NOGs\n",
    "        nog_members_df_subset = nog_members_df.head(n)\n",
    "        # create a set of all the taxa in the subset\n",
    "        taxa_in_subset = set(\n",
    "            [taxon for sublist in nog_members_df_subset['taxa'].tolist() for taxon in sublist])\n",
    "        # check if the taxon_list is a subset of taxa_in_subset\n",
    "        if set(taxon_list).issubset(taxa_in_subset):\n",
    "            print(\"At n = \", n, \", taxon_list is a subset of taxa_in_subset, with total of \", len(\n",
    "                taxa_in_subset), \" taxa\")\n",
    "            break\n",
    "        else:\n",
    "            n += 100\n",
    "\n",
    "# replace the nog_members_df with the subset, and rewrite the nog_members_df file\n",
    "nog_members_df = nog_members_df_subset\n",
    "# before writing out the nog_members_df, remove the columns that are not needed\n",
    "nog_members_df = nog_members_df.drop(columns=['diff_from_mean'])\n",
    "# also join the lists into comma-separated strings\n",
    "nog_members_df['taxa'] = nog_members_df['taxa'].apply(lambda x: ','.join(x))\n",
    "nog_members_df['genes'] = nog_members_df['genes'].apply(lambda x: ','.join(x))\n",
    "nog_members_df.to_csv(f\"{data_dir}{taxonomicID}_nog_members.tsv\",\n",
    "                  sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "\n",
    "# split the taxa and genes comma-separated strings back into lists\n",
    "nog_members_df['taxa'] = nog_members_df['taxa'].str.split(',')\n",
    "nog_members_df['genes'] = nog_members_df['genes'].str.split(',')\n",
    "\n",
    "# display the last 5 rows of the nog_members_df\n",
    "display(nog_members_df.tail())\n",
    "\n",
    "# write out the list of taxa in this subset\n",
    "with open(f\"{data_dir}{taxonomicID}_subset_taxa.txt\", \"w\") as taxa_fo:\n",
    "    taxa_fo.write('\\n'.join(taxon_list) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of trees_df:  103547\n",
      "Length of trees_df after filtering:  1300\n"
     ]
    }
   ],
   "source": [
    "# Read in the TSV file with NOG to trees mapping\n",
    "trees_df = pd.read_csv(f\"{data_dir}{taxonomicID}_all_trees.tsv\",\n",
    "                       sep=\"\\t\", names=['NOG', 'tree'])\n",
    "# Display length of the DataFrame\n",
    "print(\"Length of trees_df: \", len(trees_df))\n",
    "# Remove NOGs that are not in the nog_members_df\n",
    "trees_df = trees_df[trees_df['NOG'].isin(nog_members_df['NOG'])]\n",
    "# Display length of the new trees_df\n",
    "print(\"Length of trees_df after filtering: \", len(trees_df))\n",
    "# Overwrite the subset_trees.tsv file with the new trees\n",
    "trees_df.to_csv(f\"{data_dir}{taxonomicID}_subset_trees.tsv\",\n",
    "                sep=\"\\t\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of locus_tags considering the wol_tree_taxa: 1032613. Looks like: ['1005090.BAKON_001', '1005090.BAKON_002', '1005090.BAKON_003', '1005090.BAKON_004', '1005090.BAKON_005']\n"
     ]
    }
   ],
   "source": [
    "# read in wol_tree and make a list of taxa in the tree\n",
    "wol_tree_filepath = f'{data_dir}{taxonomicID}_wol_tree_pruned_no_internal_labels.nwk'\n",
    "wol_tree = ete3.Tree(wol_tree_filepath, format=1)\n",
    "wol_tree_taxa = wol_tree.get_leaf_names()\n",
    "\n",
    "# read in list of locus tags with features\n",
    "all_genes_list_filepath = f'{data_dir}{taxonomicID}_locus_tags_with_features.txt'\n",
    "with open(all_genes_list_filepath, 'r') as in_file:\n",
    "    all_genes_list = in_file.read().splitlines()\n",
    "\n",
    "# create a list of locus_tags that are in the wol_tree_taxa\n",
    "all_locus_tags = [locus_tag for locus_tag in all_genes_list if locus_tag.split('.', 1)[0] in wol_tree_taxa]\n",
    "print(f'Number of locus_tags considering the wol_tree_taxa: {len(all_locus_tags)}. Looks like: {all_locus_tags[:5]}')\n",
    "\n",
    "# write this to a file\n",
    "all_locus_tags_filepath = f'{data_dir}{taxonomicID}_subset_genes.txt'\n",
    "with open(all_locus_tags_filepath, 'w') as out_file:\n",
    "    out_file.write('\\n'.join(all_locus_tags) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees before pruning: 1300\n",
      "Number of trees after pruning: 1300\n"
     ]
    }
   ],
   "source": [
    "# prune trees\n",
    "# read in the subset trees\n",
    "subset_trees_filepath = f'{data_dir}{taxonomicID}_subset_trees.tsv'\n",
    "subset_trees_df = pd.read_csv(subset_trees_filepath, sep='\\t', header=None, names=['NOG', 'tree'])\n",
    "\n",
    "# testing\n",
    "# subset_trees_df = subset_trees_df.head(10)\n",
    "\n",
    "# read in the subset genes\n",
    "subset_genes_filepath = f'{data_dir}{taxonomicID}_subset_genes.txt'\n",
    "with open(subset_genes_filepath, 'r') as in_file:\n",
    "    subset_genes_list = in_file.read().splitlines()\n",
    "\n",
    "def apply_prune_tree(args):\n",
    "    \"\"\"\n",
    "    fn for multiprocessing pool to prune trees\n",
    "    \"\"\"\n",
    "    index, row, subset_genes_list = args\n",
    "    tree_newick = row['tree'].strip()\n",
    "    # read in the tree\n",
    "    tree = ete3.Tree(tree_newick, format=1)\n",
    "    # get the leaf names\n",
    "    leaf_names = tree.get_leaf_names()\n",
    "    # find the genes that are in the tree\n",
    "    leaves_to_keep = [leaf for leaf in leaf_names if leaf in subset_genes_list]\n",
    "    # print(f'Number of leaves in tree: {len(leaf_names)}, Number of leaves to keep: {len(leaves_to_keep)}')\n",
    "    # prune the tree\n",
    "    if len(leaves_to_keep) <=4:\n",
    "        return None\n",
    "    elif len(leaves_to_keep) == len(leaf_names):\n",
    "        return tree.write(format=1)\n",
    "    else:\n",
    "        tree.prune(leaves_to_keep)\n",
    "        return tree.write(format=1)\n",
    "    \n",
    "# create a list of tuples to pass to the pool\n",
    "args = [(index, row, subset_genes_list) for index, row in subset_trees_df.iterrows()]\n",
    "max_proc = 2*mp.cpu_count()\n",
    "with mp.Pool(max_proc) as pool:\n",
    "    subset_trees_df['pruned_tree'] = list(pool.map(apply_prune_tree, args))\n",
    "\n",
    "# remove rows where pruned_tree is None\n",
    "print(f'Number of trees before pruning: {len(subset_trees_df)}')\n",
    "subset_trees_df = subset_trees_df[subset_trees_df['pruned_tree'].notnull()]\n",
    "print(f'Number of trees after pruning: {len(subset_trees_df)}')\n",
    "\n",
    "# write the NOg: prune_tree mapping to a file\n",
    "pruned_trees_filepath = f'{data_dir}{taxonomicID}_pruned_gene_trees.tsv'\n",
    "subset_trees_df[['NOG', 'pruned_tree']].to_csv(pruned_trees_filepath, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxa in wol tree but not in gene trees: 1 \n",
      "and vice versa: 0\n",
      "They look like this: ['243277'] and []\n",
      "Number of taxa to keep in the wol tree: 359\n"
     ]
    }
   ],
   "source": [
    "# check if the taxa in wol tree, and taxa in set of all gene trees, are the exact same sets\n",
    "# read in the rooted gene trees file\n",
    "gene_trees_filepath = f'{data_dir}{taxonomicID}_pruned_gene_trees.tsv'\n",
    "with open(gene_trees_filepath, 'r') as in_file:\n",
    "    rooted_gene_trees = [i.split()[1].strip() for i in in_file.readlines()]\n",
    "\n",
    "# create a set of all taxa in the gene trees\n",
    "all_taxa_in_gene_trees = set()\n",
    "for tree in rooted_gene_trees:\n",
    "    gene_tree = ete3.Tree(tree, format=1)\n",
    "    all_genes = gene_tree.get_leaf_names()\n",
    "    all_taxa_in_gene_trees.update([gene.split('.', 1)[0] for gene in all_genes])\n",
    "\n",
    "# read in the wol tree\n",
    "wol_tree_no_internal_labels_filepath = f'{data_dir}{taxonomicID}_wol_tree_pruned_no_internal_labels.nwk'\n",
    "wol_tree_with_internal_labels_filepath = f'{data_dir}{taxonomicID}_wol_tree_pruned_with_internal_labels.nwk'\n",
    "wol_tree_angst_filepath = f'{data_dir}{taxonomicID}_wol_tree_pruned_angst.nwk'\n",
    "wol_tree = ete3.Tree(wol_tree_no_internal_labels_filepath, format=1)\n",
    "wol_tree_taxa = set(wol_tree.get_leaf_names())\n",
    "\n",
    "# print which taxa are in wol tree but not in gene trees and vice versa\n",
    "print(f'Taxa in wol tree but not in gene trees: {len(wol_tree_taxa - all_taxa_in_gene_trees)} \\nand vice versa: {len(all_taxa_in_gene_trees - wol_tree_taxa)}')\n",
    "print(f'They look like this: {list(wol_tree_taxa - all_taxa_in_gene_trees)[:5]} and {list(all_taxa_in_gene_trees - wol_tree_taxa)[:5]}')\n",
    "\n",
    "# prune all the variants of the wol_tree to only include the taxa in the gene trees\n",
    "wol_tree_taxa_to_keep = wol_tree_taxa.intersection(all_taxa_in_gene_trees)\n",
    "print(f'Number of taxa to keep in the wol tree: {len(wol_tree_taxa_to_keep)}')\n",
    "# prune the wol tree to only include the taxa in the gene trees\n",
    "wol_tree.prune(wol_tree_taxa_to_keep)\n",
    "wol_tree.write(outfile=wol_tree_no_internal_labels_filepath, format=1, format_root_node=True, dist_formatter='%.10f')\n",
    "\n",
    "# write a version of the tree with root length for AnGST.\n",
    "# first make a copy of the tree\n",
    "wol_tree.write(outfile=wol_tree_angst_filepath,  \n",
    "               format=1, dist_formatter='%f', format_root_node=True)\n",
    "# find the last \")\" in this file and replace everything after it with \":0.0);\".\n",
    "# AnGST doesn't like the branch length at the root to be like 0.00000\n",
    "with open(wol_tree_angst_filepath, \"r\") as fi:\n",
    "    tree_str = fi.read()\n",
    "    tree_str = tree_str[:tree_str.rfind(\")\")+1] + \":0.0);\"\n",
    "with open(wol_tree_angst_filepath, \"w\") as fo:\n",
    "    fo.write(tree_str)\n",
    "\n",
    "\n",
    "# read in the wol tree with internal labels, prune it, and overwrite it\n",
    "wol_tree = ete3.Tree(wol_tree_with_internal_labels_filepath, format=1)\n",
    "wol_tree.prune(wol_tree_taxa_to_keep)\n",
    "# relabel the internal node names\n",
    "internal_node_index = 1\n",
    "for node in wol_tree.traverse(strategy=\"postorder\"):\n",
    "    if not node.is_leaf():\n",
    "        node.name = f\"N{internal_node_index}\"\n",
    "        internal_node_index += 1\n",
    "wol_tree.write(outfile=wol_tree_with_internal_labels_filepath, format=1, format_root_node=True, dist_formatter='%.10f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a new NOG members file for the subset of the NOGs and the genes in the subset\n",
    "# read in original NOG members file\n",
    "nog_members_tsv_filepath = f\"{data_dir}{taxonomicID}_nog_members.tsv\"\n",
    "nog_members_df = pd.read_csv(nog_members_tsv_filepath, sep='\\t', header=None, names=['taxonomic_group',\n",
    "                                                                                    'NOG',\n",
    "                                                                                    '#taxa',\n",
    "                                                                                    '#genes',\n",
    "                                                                                    'taxa', 'genes', '#genes_to_keep'])\n",
    "# remove NOGs not in subset_trees_df\n",
    "subset_trees_df = pd.read_csv(f\"{data_dir}{taxonomicID}_pruned_gene_trees.tsv\", sep=\"\\t\", header=None, names=['NOG', 'tree'])\n",
    "nog_members_df = nog_members_df[nog_members_df['NOG'].isin(subset_trees_df['NOG'])]\n",
    "\n",
    "# in `taxa` column keep only taxa that are in wol_tree_taxa_to_keep\n",
    "nog_members_df['taxa'] = nog_members_df['taxa'].apply(\n",
    "    lambda taxa: [taxon for taxon in taxa.split(',') if taxon in wol_tree_taxa_to_keep])\n",
    "# in `genes` column keep only genes that are from taxa in wol_tree_taxa_to_keep\n",
    "nog_members_df['genes'] = nog_members_df['genes'].apply(\n",
    "    lambda genes: [gene for gene in genes.split(',') if gene.split('.', 1)[0] in wol_tree_taxa_to_keep])\n",
    "# update the #taxa and #genes columns by counting the length of the lists in `taxa` and `genes` columns\n",
    "nog_members_df['#taxa'] = nog_members_df['taxa'].apply(len)\n",
    "nog_members_df['#genes'] = nog_members_df['genes'].apply(len)\n",
    "# drop the #genes_to_keep column\n",
    "nog_members_df = nog_members_df.drop(columns=['#genes_to_keep'])\n",
    "\n",
    "# join the lists into comma-separated strings\n",
    "nog_members_df['taxa'] = nog_members_df['taxa'].apply(lambda x: ','.join(x))\n",
    "nog_members_df['genes'] = nog_members_df['genes'].apply(lambda x: ','.join(x))\n",
    "\n",
    "# overwrite the NOG members file\n",
    "nog_members_df.to_csv(f\"{data_dir}{taxonomicID}_nog_members.tsv\", sep=\"\\t\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root all of these gene trees using MAD\n",
    "\n",
    "The below command is what you can run and then wait to finish. It will be run in the background (because of `nohup` and `disown`) even if you lose connection to the server. This process can take in the order of hours to finish, for a list of trees in the order of 10k trees.\n",
    "\n",
    "```\n",
    "# cd 01-prepare_data\n",
    "$ nohup python src/run_MAD_on_EggNOG_parallel.py -i ../data/1236_pruned_gene_trees.tsv -m ~/bin/mad/mad -n 100 > ../data/nohup_mad_eggnog_rooting_1236.log & disown\n",
    "```\n",
    "\n",
    "**NOTE**: Pruning took roughly 1.5 hours for 15k trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download genome sequence files for each of the taxa in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "nohup python src/download_ncbi_genome_sequences.py -i ../data/1236_subset_taxa.txt  --to-download genome -a ../data/genome_data/1236_all_accession_ids.tsv > nohup_genome_dload.log & disown\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of genome accessions: 1473, and they look like:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genome_accession</th>\n",
       "      <th>fna_filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GCF_000953635.1</td>\n",
       "      <td>/root/work/projects/paper_comparative_study_of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GCF_900461565.1</td>\n",
       "      <td>/root/work/projects/paper_comparative_study_of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GCF_900450575.1</td>\n",
       "      <td>/root/work/projects/paper_comparative_study_of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GCF_015476275.1</td>\n",
       "      <td>/root/work/projects/paper_comparative_study_of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GCF_000953655.1</td>\n",
       "      <td>/root/work/projects/paper_comparative_study_of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  genome_accession                                       fna_filepath\n",
       "0  GCF_000953635.1  /root/work/projects/paper_comparative_study_of...\n",
       "1  GCF_900461565.1  /root/work/projects/paper_comparative_study_of...\n",
       "2  GCF_900450575.1  /root/work/projects/paper_comparative_study_of...\n",
       "3  GCF_015476275.1  /root/work/projects/paper_comparative_study_of...\n",
       "4  GCF_000953655.1  /root/work/projects/paper_comparative_study_of..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all the files downloaded into a TSV file with columns genome_accession and fna_filepath\n",
    "genome_data_dir = f'{data_dir}/genome_data/'\n",
    "genome_data_dir_fullpath = os.path.abspath(genome_data_dir)\n",
    "genome_data_files = [i for i in os.listdir(genome_data_dir) if i.endswith('.fna')]\n",
    "# filename basenames are of the form {taxonID}_{GCA_xxxxxxx.n}_{genomeAssembly}_genomic.fna\n",
    "# we want to extract the genome accession from this\n",
    "genome_accession_filepath_tuples_list = [('_'.join(i.split('_')[1:3]), os.path.join(genome_data_dir_fullpath, i)) for i in genome_data_files]\n",
    "genome_accessions_filepaths_df = pd.DataFrame(genome_accession_filepath_tuples_list, columns=['genome_accession', 'fna_filepath'])\n",
    "genome_accessions_filepaths_df.to_csv(f'{data_dir}{taxonomicID}_genome_fna_filepaths.tsv', sep='\\t', index=False, header=True)\n",
    "print(f'Number of genome accessions: {len(genome_accessions_filepaths_df)}, and they look like:')\n",
    "display(genome_accessions_filepaths_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in this rooted pruned trees file, and replace the leaf labels\n",
    "# such that taxID.geneID_geneID2 is replaced with taxID_geneID_geneID2\n",
    "\n",
    "# function to replace first underscore with dot in leaf label\n",
    "def replace_first_dot_with_underscore_in_leaf_label(tree_line):\n",
    "    tree_ID, tree = tree_line.split('\\t')\n",
    "    gene_tree = ete3.Tree(tree, format=1)\n",
    "    for leaf in gene_tree:\n",
    "        leaf.name = leaf.name.replace('.', '_', 1)\n",
    "    # add a branch length of 0.0 to the root node\n",
    "    gene_tree.get_tree_root().dist = 0.0\n",
    "    return tree_ID+'\\t'+gene_tree.write(format=1, dist_formatter='%f', format_root_node=True)\n",
    "\n",
    "\n",
    "# create a pool of workers\n",
    "max_workers = 500\n",
    "pool = mp.Pool(max_workers)\n",
    "\n",
    "pruned_rooted_gene_trees_tsv_filepath = f'{data_dir}{taxonomicID}_pruned_gene_trees.tsv.rooted'\n",
    "# open both the input and output file and apply the function to each line in the input file\n",
    "with open(pruned_rooted_gene_trees_tsv_filepath) as infile, open(f\"{pruned_rooted_gene_trees_tsv_filepath}.underscored\", 'w') as outfile:\n",
    "    # iterate over each line in the input file\n",
    "    for line in infile:\n",
    "        # apply the function to the line and write the result to the output file\n",
    "        result = pool.apply_async(\n",
    "            replace_first_dot_with_underscore_in_leaf_label, (line.strip(),))\n",
    "        outfile.write(result.get() + '\\n')\n",
    "\n",
    "# close the pool\n",
    "pool.close()\n",
    "pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a TSV file with gene ID, start position, end position, strand, and genome accession ID\n",
    "# for gene IDs in the gene features json for the subset of genes in the gene trees\n",
    "\n",
    "def return_gene_features(args):\n",
    "    \"\"\"\n",
    "    generator fn to return gene features for a taxon as a list\n",
    "    \"\"\"\n",
    "    taxon, genes_of_taxon, taxon_gene_features_dict = args\n",
    "    gene_features_lists = []\n",
    "    for gene in genes_of_taxon:\n",
    "        if gene in taxon_gene_features_dict['genes']:\n",
    "            try:\n",
    "                gene_features = taxon_gene_features_dict['genes'][gene]\n",
    "                gene_features_lists.append([gene, gene_features['id'], \n",
    "                       gene_features['start'], gene_features['end'],\n",
    "                       gene_features['strand'], gene_features['seqid'],\n",
    "                       taxon_gene_features_dict['acc_id'], taxon])\n",
    "            except KeyError:\n",
    "                print(f'KeyError for gene {gene} in taxon {taxon}, with accesion {taxon_gene_features_dict[\"acc_id\"]}')\n",
    "    return gene_features_lists\n",
    "\n",
    "with open(f'{data_dir}{taxonomicID}_gene_features.json', 'r') as in_file:\n",
    "    gene_features_dict = json.load(in_file)\n",
    "\n",
    "with open(f'{data_dir}{taxonomicID}_subset_taxa.txt', 'r') as in_file:\n",
    "    subset_taxa_list = in_file.read().splitlines()\n",
    "\n",
    "args = [(taxon, list(gene_features_dict[taxon]['genes'].keys()), gene_features_dict[taxon]) for taxon in subset_taxa_list]\n",
    "\n",
    "max_workers = len(args)\n",
    "\n",
    "gene_features_tsv_filepath = f'{data_dir}{taxonomicID}_gene_features.tsv'\n",
    "header_list = ['locus_tag', 'gene_id', 'start', 'end', 'strand', 'seqid', 'genome_accession', 'taxon_id']\n",
    "\n",
    "BUFFER_SIZE = 1000  # Keeping a buffer to write to file in chunks\n",
    "# the buffer is important otherwise IOStream will be the bottleneck\n",
    "\n",
    "def write_buffer(buffer, out_file):\n",
    "    out_file.write('\\n'.join(buffer) + '\\n')\n",
    "    buffer.clear()\n",
    "\n",
    "with open(gene_features_tsv_filepath, 'w') as out_file:\n",
    "    # write the header\n",
    "    out_file.write('\\t'.join(header_list) + '\\n')\n",
    "    # as you get the results, write them to the file instead of storing them in memory\n",
    "    buffer = []\n",
    "    with mp.Pool(max_workers) as pool:\n",
    "        for result in pool.imap(return_gene_features, args):\n",
    "            for gene_features in result:\n",
    "                buffer.append('\\t'.join(map(str, gene_features)))\n",
    "                if len(buffer) >= BUFFER_SIZE:\n",
    "                    write_buffer(buffer, out_file)\n",
    "        if buffer:\n",
    "            write_buffer(buffer, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Presence-Absence (PA) matrices for GLOOME and Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The taxa list looks like this: ['1005057', '1005058', '1005090', '1006000', '1009858', '1026882', '1028989', '1033802', '1036674', '1045855'] ...\n",
      "The size of the pa_df is: (359, 1300)\n"
     ]
    }
   ],
   "source": [
    "def nogs_pa_matrix(taxonomicID) -> None:\n",
    "    '''\n",
    "    use the members.tsv file of the NOGs from eggNOG, to prepare the pa_matrix\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    # read in the members.tsv file, but only the NOG, and the taxa columns (column 1 and 4)\n",
    "    members_df_subset = pd.read_csv(f\"{data_dir}/{taxonomicID}_nog_members.tsv\", sep='\\t', header=None,\n",
    "                                    names=['NOG', 'taxa', 'genes'], usecols=[1, 4, 5])\n",
    "    # split the taxa column into a list of taxa\n",
    "    members_df_subset['taxa'] = members_df_subset['taxa'].apply(\n",
    "        lambda x: x.split(','))\n",
    "    # similar for genes column\n",
    "    members_df_subset['genes'] = members_df_subset['genes'].apply(\n",
    "        lambda x: x.split(','))\n",
    "\n",
    "    # create pa matrix based on the presence-absence of the taxa in the NOGs\n",
    "    # first create a list of all the taxa\n",
    "    taxa = list(\n",
    "        set([item for sublist in members_df_subset['taxa'].tolist() for item in sublist]))\n",
    "    # sort the list\n",
    "    taxa.sort()\n",
    "    # display how the list looks like\n",
    "    print(\"The taxa list looks like this: {}\".format(taxa[:10]), \"...\")\n",
    "\n",
    "    # create a dataframe with the NOGs as columns and the taxa as index\n",
    "    # fill the dataframe with 0s as strings\n",
    "    pa_df = pd.DataFrame(\n",
    "        index=taxa, columns=members_df_subset['NOG'], dtype=str).fillna('0')\n",
    "\n",
    "    # fill in the presence-absence matrix based on whether the taxa are present in the list of taxa for each NOG\n",
    "    for index, row in members_df_subset.iterrows():\n",
    "        for taxon in row['taxa']:\n",
    "            # find how many (\"_\"-delimited) genes in the corresponding gene column, have the taxon as the first element\n",
    "            # (i.e. the taxon is the first element of the gene name)\n",
    "            pa_df.loc[taxon, row['NOG']] = str(\n",
    "                int(len([gene for gene in row['genes'] if gene.split('.', 1)[0] == taxon])))\n",
    "\n",
    "    # display the size of the matrix/df\n",
    "    print(\"The size of the pa_df is: {}\".format(pa_df.shape))\n",
    "\n",
    "    # TSV file for COUNT, with NOGs in index and taxa as columns\n",
    "    pa_df.T.to_csv(f\"{data_dir}/{taxonomicID}_pa_matrix.tsv\", sep='\\t')\n",
    "\n",
    "    # now binarize the matrix (i.e. if a NOG has at least one gene from a taxon, it gets a 1, otherwise 0)\n",
    "    pa_df = pa_df.map(lambda x: '1' if x != '0' else '0')\n",
    "\n",
    "    # Concatenate the columns into a single string per taxon and write out as FASTA file\n",
    "    fasta_df = pd.DataFrame(pa_df.astype(str).apply(\n",
    "        ''.join, axis=1), columns=['sequence'])\n",
    "    fasta_df.index = '>' + fasta_df.index.str.strip()\n",
    "    fasta_df.to_csv(\n",
    "        f\"{data_dir}{taxonomicID}_pa_matrix.fasta\", sep='\\n', header=False)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# run function\n",
    "nogs_pa_matrix(taxonomicID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_dir\"\n",
    "# extract the function profiles for the NOGs in our dataset from the eggnog6 database\n",
    "\n",
    "# remove entries with no function profile\n",
    "rg -v \"\\{\\}\" $1\"/eggnog6/e6.func_profiles.json\" > $1\"/eggnog6/e6.func_profiles.available.json\"\n",
    "# get function profiles for nog members in our dataset\n",
    "rg -f <(cut -f2 $1\"/1236_nog_members.tsv\" )  $1\"/eggnog6/e6.func_profiles.available.json\" > $1\"/1236.func_profiles.json\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hgt_analyses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
